{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval\n",
    "\n",
    "- Many LLM applications require user-specific data outside of the model training set.\n",
    "- E.g. ask a question on some documents that are private to the company.\n",
    "- How do we do? Two simples approaches one could think of could be:\n",
    "  - Option 1: Fine-tune the LLM on the new data.\n",
    "  - Option 2: Pass the internal documents as part of the prompt.\n",
    "- However, both of these approaches have drawbacks:\n",
    "  - Option 1\n",
    "    - Fine-tuning is expensive (both in terms of time and money)\n",
    "    - What if we want to add more documents later?\n",
    "    - In general, remember: A more useful way to think of LLMs is to think of them as **reasoning engines**, not assume they know everything!\n",
    "  - Option 2\n",
    "    - Now we are on the right track!\n",
    "    - But what if the concatenation of all documents is too long for the context length of the LLM?\n",
    "    - And even if it fits in the context length, maybe it is not the most efficient way to do it (we pay per tokens: USD 0.03/1k prompt tokens + USD 0.06/1k sampled tokens).\n",
    "- So the idea is to apply **retrieval augmented generation** (RAG).\n",
    "  - Fundamentally, same architecture of classical Information Retrieval (IR) systems.\n",
    "  - But Reader is replaced by LLM: like Option 2, but prompt is just set of relevant paragraphs.\n",
    "- Reminder, the fundamental structure of an IR system is:\n",
    "  - **Indexing**: Create an index of the documents/paragraphs. Historically, using `FULLTEXT INDEX` in MySQL, then more advanced tools like [Elasticsearch](https://www.elastic.co/), and more recently **Dense Passage Retrieval** i.e. Deep Learning (Sentence Transformers, GPT embeddings, ...) based vector embeddings.\n",
    "  - **Retriever**: Given a new query, retrieve the most relevant documents/paragraphs. As we now use vector embeddings, this is done by computing the cosine similarity between the query embedding and all the precomputed document embeddings (runtime is typically very fast, as approximate neighbor search with clustering, dimensionality reduction, and other techniques can be used).\n",
    "  - **Re-Ranker**: The goal of Retrieval is to very quickly fetch from a huge number of documents, a handful of potentially relevant documents: so speed is key, not accuracy. The goal of the Re-Ranker is to very accurately re-rank the documents. This can be done e.g. with a Cross-Encoder, that takes as input the query and the document, and outputs a score. Why not directly use the Cross-Encoder for Retrieval? Because it is too slow! Why is Bi-Encoder so fast? Because only 1 embedding is computed per query, all the document embeddings are pre-computed!\n",
    "  - **Reader**: Given the top-k documents, the Reader will read them and extract the answer. This means that we have a context = top-k documents, and we can use that as the prompt (with the question) for the Reader. Historically, the Reader was e.g. a BERT model fine-tuned on SQuAD v2. Now, we can use a LLM, and the prompt is the concatenation of the question and the top-k documents.\n",
    "- Even if it doesn't necessarily involve LLMs (e.g. you can use Sentence Transformers embeddings instead of GPT embeddings), the Indexing is key to the success of the application. In particular fast retrieval is key when the number of documents grows huge. Hence, vector databases have become very popular!\n",
    "  - Pinecone\n",
    "  - Weaviate\n",
    "  - Milvus\n",
    "  - Qdrant\n",
    "  - ...\n",
    "\n",
    "- How does LangChain fit in these picture? It provides implementation for all the main components!\n",
    "  - **Document Loaders** — Load documents from any source (HTML, PDF, source code, ...) and location (local, S3, websites, ...).\n",
    "  - **Document Transformers** — Split documents into smaller chunks, e.g. paragraphs, sentences, code blocks, ...\n",
    "  - **Text Embedding Models** — Create embeddings for the text chunks, e.g. Sentence Transformers, GPT, ...\n",
    "  - **Vector Stores** — Store the embeddings in a vector database, e.g. Pinecone, Weaviate, Milvus, Qdrant, ...\n",
    "  - **Retrievers** — Retrieve the most relevant documents given a query, e.g. from Simple Semantic Search to more advanced algorithms like Parent Document Retriever, Self Query Retriever, Ensemble Retriever, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Copy .env.example file as .env: `cp .env.example .env`\n",
    "# 2. Open .env file and set all the env variables\n",
    "load_dotenv(\".env\")\n",
    "OPENAI_KEY = os.getenv(\"OPENAI_KEY\")\n",
    "assert OPENAI_KEY, \"Please set your OPENAI_KEY environment variable.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "from langchain.embeddings import OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Querying Textual Documents with Vector Stores\n",
    "\n",
    "- This is the most common application.\n",
    "- We have a question, and we want to find the answer based on a number of textual documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Document Loader\n",
    "file_path = \"data/state_of_the_union_2023.txt\"\n",
    "document_loader = TextLoader(file_path=file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Embedding Model\n",
    "embedding_model = OpenAIEmbeddings(openai_api_key=OPENAI_KEY, model=\"text-embedding-ada-002\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Index\n",
    "index = VectorstoreIndexCreator(\n",
    "    vectorstore_cls=FAISS,\n",
    "    embedding=embedding_model,\n",
    ").from_loaders([document_loader])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Reader Model\n",
    "chat_model = ChatOpenAI(\n",
    "    openai_api_key=OPENAI_KEY,\n",
    "    model_name=\"gpt-4\",\n",
    "    temperature=0,\n",
    "    model_kwargs={\"top_p\":1},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Putin's invasion was a test for America and the world, challenging principles of sovereignty and freedom from tyranny.\n",
      "- America, along with NATO and a global coalition, stood against Putin's aggression and in support of the Ukrainian people.\n",
      "- The President spoke about the situation one year after Putin unleashed a brutal war against Ukraine, comparing it to the death and destruction Europe suffered in World War II.\n",
      "- The President affirmed that if any country, including Russia, threatens America's sovereignty, they will act to protect their country.\n"
     ]
    }
   ],
   "source": [
    "response = index.query(\"Summarize in bullet points what the President said about Putin.\", llm=chat_model)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Querying a Graph with NetworkX\n",
    "\n",
    "- Sometimes our data is in a graph format, and we want to query it.\n",
    "- We can use NetworkX to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new GraphQAChain chain...\u001b[0m\n",
      "Entities Extracted:\n",
      "\u001b[32;1m\u001b[1;3mIrene Isaac\u001b[0m\n",
      "Full Context:\n",
      "\u001b[32;1m\u001b[1;3mIrene Isaac works_at Ford\n",
      "Irene Isaac lives_in GreenVillage\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Irene Isaac works at Ford.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import GraphQAChain\n",
    "from langchain.indexes.graph import NetworkxEntityGraph\n",
    "\n",
    "file_path = \"./data/people_and_jobs_graph.gml\"\n",
    "graph = NetworkxEntityGraph.from_gml(file_path)\n",
    "\n",
    "chain = GraphQAChain.from_llm(llm=chat_model, graph=graph, verbose=True)\n",
    "\n",
    "chain.run(\"Where does Irene Isaac work?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new GraphQAChain chain...\u001b[0m\n",
      "Entities Extracted:\n",
      "\u001b[32;1m\u001b[1;3mGreenVillage\u001b[0m\n",
      "Full Context:\n",
      "\u001b[32;1m\u001b[1;3mGreenVillage is_in Illinois\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'GreenVillage is located in Illinois.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.run(\"Where is GreenVillage located?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new GraphQAChain chain...\u001b[0m\n",
      "Entities Extracted:\n",
      "\u001b[32;1m\u001b[1;3mIrene Isaac\u001b[0m\n",
      "Full Context:\n",
      "\u001b[32;1m\u001b[1;3mIrene Isaac works_at Ford\n",
      "Irene Isaac lives_in GreenVillage\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"The text doesn't provide information on the location of the company where Irene Isaac works.\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.run(\"Where is the company where Irene Isaac works located?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tmp-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
